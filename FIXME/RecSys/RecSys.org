#+TITLE: 推荐系统
#+AUTHOR: stupid-coder
#+EMAIL: stupid_coder@163.com
#+OPTIONS: num:nil H:2
#+STARTUP: indent


* 简介
#+BEGIN_QUOTE
推荐系统主要依赖于用户历史上和推荐物料之间的交互行为来进行未来物料推荐的系统.
#+END_QUOTE

* 推荐算法
** Collaborative Filtering
*** Neighborhood-Based Collaborative Filtering
基于近邻协同过滤算法又叫做基于内存的算法(/memory-based/).

**** User-Based collaborative filtering
通过计算和当前用户相似用户群,然后根据相似用户对 Item 的行为来进行推测.

***** 用户相似性度量
用户相似性一般采用皮尔逊相关系数来表示.

+ 首先计算用户的投票均值 ::
                 \begin{equation}
                   \mu_{u} = \frac{\sum_{k \in I_{u}}r_{uk}}{\|I_{u}\|} \forall{u} \in {1...m}
                 \end{equation}
                 $I_{u}$ 为用户 u 投票的 Item 的集合. $r_{uk}$ 表示用户 u 对 Item k 的投票值.

+ 计算皮尔逊相关系数 :: 
               \begin{equation}
                 Sim(u,v) = Person(u,v) = \frac{\sum_{k\in I_{u} \cap I_{v}}(r_{uk}-\mu_{u})(r_{vk}-\mu_{k})}{\sqrt{\sum_{k\in I_{u} \cap I_{v}}(r_{uk}-\mu_{u})^{2}}\sqrt{\sum_{k\in I_{u} \cap I_{v}}(r_{vk}-\mu_{v})^{2}}}
               \end{equation}
               常规的皮尔逊系数中的均值也是要根据两个用户投票的集合来计算的,这样需要计算两两用户的交互 Item 的投票均值,带来大量的计算.所以这里采用的是用户的全局投票均值. *没有任何证明表示采用全局的均值要比标准的要差.*

***** 根据 top-k 相似用户预估投票值
可以直接对 top-k 的相似用户对某个 Item 的投票值进行皮尔逊系数的加权求和来预估投票值:
\begin{equation}
  \hat{r_{uk}}= \frac{\sum_{v \in |top-k users|}Sim(u,v)r_{vk}}{\sum_{v \in |top-k users|}Sim(u,v)}
\end{equation}

上述的计算方法主要问题是不用用户的投票值具有不同的尺度.有的用户喜欢打高分,有的用户比较严格打分较低.可以对用户的投票值进行中心化(/mean-centered/):

#+NAME: 去中心化
\begin{equation}
  s_{u,j}=r_{uj}-\mu_{u} \ \forall u \in {1,...,m}
\end{equation}

通过相关性加权求和后,同时需要将当前用户的投票均值加上得到最后的投票得分:
\begin{equation}
  \hat{r_{uj}} = \mu_{u} + \frac{\sum_{v \in P_{u}(j)}Sim(u,v)s_{vj}}{\sum_{v \in P_{u}(j)}|Sim(u,v)|} = \mu_{u} + \frac{\sum_{v \in P_{u}(j)}Sim(u,v)(r_{vj}-\mu_{v})}{\sum_{v \in P_{u}(j)}|Sim(u,v)|}
\end{equation}
$P_{u}(j)$ 为与用户 u 最相似的 top-k 且对 Item j 有投票得分的用户集合.

*top-k 个相似用户,同时可以考虑相似性过滤,过滤掉相似性较低的用户来提高结果的置信度.*

***** 其他相似性度量函数

+ 余玄函数 ::
          \begin{equation}
            RawCosine(u, v) = \frac{\sum_{k\in I_{u} \cap I_{v}}r_{uk}r_{vk}}{\sqrt{\sum_{k\in I_{u} \cap I_{v}}r_{uk}^{2}}\sqrt{\sum_{k\in I_{u} \cap I_{v}}r_{vk}^{2}}}
          \end{equation}
+ 打折相似函数 ::
            由于在计算相似度的时候只考虑两个用户交叉的 Items.两个用户交叉的 Items 越多,两个用户的相似性度量的置信度越高.例如如果两个用户只有一个交叉的 Item,那么不管打分如何相似度都是 1,这显然是有问题的.
            可以引入打折因子来对交互较少的用户进行衰减.
            \begin{equation}
              DiscountedSim(u,v) = Sim(u,v) \frac{min{|I_{u} \cap I_{v}|, \beta}}{\beta}
            \end{equation}

***** 其他的预估函数
除了可以采用去中心值来对用户的投票值进行一个平移去除整体的偏移,还可以对去中心后的投票值进行 /Z-score/ 归一化,即将去中心后的值 $s_{uj}$ 除以标准差 $\sigma_{u}$,标准差定义如下:
\begin{equation}
  \sigma_{u} = \sqrt{\frac{\sum_{j \in I_{u}}(r_{uj}-\mu_{u})^{2}}{|I_{u}-1|}} \forall u \in {1,...,m}
\end{equation}

则,标准化后的投票值为:
\begin{equation}
  z_{uj} = \frac{r_{uj}-\mu_{u}}{\sigma_{u}} = \frac{s_{uj}}{\sigma_{u}}
\end{equation}

设 $P_{u}(j)$ 为与目标用户 u 相似度最高的 top-k 用户集,且该 k 个用户都对 item j 有投票行为.则目标用户 u 对 item j 的预估投票值:
\begin{equation}
  \hat{r_{uj}} = \mu_{u} + \sigma_{u} \frac{\sum_{v \in P_{u}(j)}Sim(u,v)z_{vj}}{\sum_{v \in P_{u}(j)}|Sim(u,v)|}
\end{equation}

-----
在进行预估的时候,对于相关性系数可以采用指数增强的方法,来增大相关性高的比重:
\begin{equation}
  Sim(u,v) = Pearson(u,v)^{\alpha}
\end{equation}

***** 过滤策略
一般设置 top-k 中的相似用户作为目标用户的相似用户集,但是 top-k 中的用户无法保证都是与目标具有强相关性的用户.所以需要将相关性较低或者负相关的用户过滤掉.

***** 长尾效应(Long-Tail)
推荐中,大量的 Item 是没有多少人进行投票的;同时也有一些 Item 是很多人都投票的,那么在计算用户相似度或预估投票值的时候,这些 Item 是没有区分度的.

可以采用类似 NLP 中的 tf-idf 的思想来对投票较少的 Item 进行加权:
\begin{equation}
  w_{j}=\log{(\frac{m}{m_{j}})} \forall j \in {1...n}
\end{equation}

在计算相似性和预估投票值的时候,采用上述公式来对 Item 进行加权.例如,计算 Pearson 相关性时:
\begin{equation}
  Pearson(u,v) = \frac{\sum_{k \in I_{u} \cap I_{v}}{w_{k}\centerdot(r_{uk}-\mu_{u})\centerdot(r_{vk}-\mu_{v})}}{\sqrt{\sum_{k\in I_{u} \cap I_{v}} w_{k}(r_{uk}-\mu_{u})^2}\centerdot\sqrt{\sum_{k \in {I_{u} \cap I_{v}}} w_{k} \centerdot (r_{vk}-\mu_{v})^2}}
\end{equation}

**** Item-Based collaborative filtering
通过计算和当前 ItemA 相似的 Items,然后根据当前用户对这些相似 Items 的行为来推断对 ItemA 的预测行为.

在计算 Item 的相似性前,首先需要对用户的投票值进行去中心化操作.参考[[%E5%8E%BB%E4%B8%AD%E5%BF%83%E5%8C%96][去中心化]].

去中心化后,基于余弦相似性计算如下:
#+NAME: Item 余弦
\begin{equation}
  AdjustedCosine(i,j) = \frac{\sum_{u \in U_{i} \cap U_{j}} \centerdot s_{ui} \centerdot s_{uj}}{\sqrt{\sum_{u\in U_{i} \cap U_{j}}s_{ui}^2}\centerdot\sqrt{\sum_{u\in U_{i} \cap U_{j}}s_{uj}^2}}
\end{equation}

Item-based 的协同过滤算法首先需要确定与目标最相似的 top-k Items 集合.设与目标 Item t 的最相似 top-k 的且用户 u 具有投票的 Item 集合为 $Q_{t}(u)$.那么预测算法如下:
#+NAME: Item-based 预估算法
\begin{equation}
  \hat{r}_{ut} = \frac{\sum_{j\in Q_{t}(u)}AdjustedCosine(j,t)\centerdot r_{uj}}{\sum_{j\in Q_{t}(u)}|AdjustedCosine(j,t)|}
\end{equation}

**** Comparing User-Based and Item-Based Methods

+ Item-Based 由于利用的是用户的投票信息来进行推荐,常常更能表现出出用户实际的兴趣
+ Item-Based 更容易提供一个推荐理由
+ Item-Based 方法更稳定,因为在推荐系统中常常用户的数量要远大于 Item 的数量,所有 Item 的相关性度量常常更具有稳定性.
+ User-Based 方法推荐出的 Item 更有多样性

  
*** Clustering and Neighborhood-Based Methods
基于最近邻的方法需要计算每个用户或者每个 Item 的最相似的 k 个对象,该过程需要消耗大量的计算,在用户量或者 Item 量较大时,不再具有实际引用价值.

聚类(/Clustering-based/)的主要思想是将 k 近邻的相似对象计算过程替换成聚类过程.k 近邻需要计算每个用户或者每个 Item 的 k 近邻对象,而聚类先将对象聚类成若干个簇,然后相似的投票预估算法应用在同一个簇里.由于 k 近邻的计算过程只应用在聚类后的簇里,所以计算对象大大减少.

聚类算法可以采用 k-means 聚类,由于推荐场景下的投票矩阵常常数据不完整,所以可以调整一下 k-means 中聚类计算公式,采用 Manhattan 距离除以参与计算的纬度来保证不完整下的距离度量相对可比.


*** Dimensionality Reduction and Neighborhood Methods
降维可以提高基于近邻方法的效果和效率.将高维稀疏数据通过降维技术映射到低维稠密表达,一般降维都是假设具有一个隐因子,所以也叫做潜在因子模型.这样即使两个用户只有少量的交互投票信息,在低维空间上也可以有效的计算距离.

潜在因子模型在推荐场景下一般有如下两个方面的用途:
+ 降维可以从行式(压缩 Item 维度)和列式(压缩用户维度)两个方向进行.对应着 User-based 和 Item-based 的协同过滤算法
+ 潜在表达从行和列空间一起进行压缩,从而从压缩后的行因子和列因子能够重构投票矩阵,从而实现缺失数据的预估


常见降维方法有主成份分析和 SVD 分解.

**** PCA 
+ 对 m*n 的投票矩阵 R 执行去中心化操作 $\hat{R}$,求列均值然后然后去中心化.
+ 求协方差均阵 $S=\hat{R}^{T}\hat{R}$
+ 求协方差矩阵的特征值和特征向量,取最大的 d 个特征向量组成变换矩阵 $P_{d}$
+ 计算低维表示: $\hat{R} P_{d}$

**** SVD 分解
+ 对 m*n 投票矩阵 R 进行数据补全,用行数据的均值作为缺失数据的值(采用用户投票均值表示缺失值)或者用列数据的均值作为缺失数据的值(采用 Item 的投票均值表示缺失置),得到结果矩阵 $R_{f}$
+ 计算 n*n 的 Item 对的相似矩阵: $S=R_{f}^{T}R_{f}$,该矩阵为半正定矩阵
+ 对矩阵 S 执行对角化:
  \begin{equation}
    S = P\Delta P^{T}
  \end{equation}
  P 为 n*n 矩阵,列向量 S 矩阵的标准正交特征向量. $\Delta$ 对角元素为 S 的非负特征向量.设 P_{d} 为 n*d 的矩阵,该矩阵只包含有 P 的前 d 大个特征向量.
+ 计算低维表示:
  \begin{equation}
    R_f P_d
  \end{equation}

  #+BEGIN_QUOTE
  对数据进行去中心化操作,有利于降低数据的偏置.

  可以先对数据进行行去中心化操作,然后沿着列去中心化操作,然后采用 SVD 进行低维变换,效果最好.
  #+END_QUOTE


**** 


