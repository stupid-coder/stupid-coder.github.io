<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="zh-CN" xml:lang="zh-CN">
<head>
<title>deep learning of binary hash codes for fast image retrieval</title>
<!-- 2018-08-07 Tue 12:59 -->
<meta  http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta  name="generator" content="Org-mode" />
<meta  name="author" content="qixiang@staff.sina.com.cn" />
<script type="text/javascript">
/*
@licstart  The following is the entire license notice for the
JavaScript code in this tag.

Copyright (C) 2012-2013 Free Software Foundation, Inc.

The JavaScript code in this tag is free software: you can
redistribute it and/or modify it under the terms of the GNU
General Public License (GNU GPL) as published by the Free Software
Foundation, either version 3 of the License, or (at your option)
any later version.  The code is distributed WITHOUT ANY WARRANTY;
without even the implied warranty of MERCHANTABILITY or FITNESS
FOR A PARTICULAR PURPOSE.  See the GNU GPL for more details.

As additional permission under GNU GPL version 3 section 7, you
may distribute non-source (e.g., minimized or compacted) forms of
that code without the copy of the GNU GPL normally required by
section 4, provided you include this license notice and a URL
through which recipients can access the Corresponding Source.


@licend  The above is the entire license notice
for the JavaScript code in this tag.
*/
<!--/*--><![CDATA[/*><!--*/
 function CodeHighlightOn(elem, id)
 {
   var target = document.getElementById(id);
   if(null != target) {
     elem.cacheClassElem = elem.className;
     elem.cacheClassTarget = target.className;
     target.className = "code-highlighted";
     elem.className   = "code-highlighted";
   }
 }
 function CodeHighlightOff(elem, id)
 {
   var target = document.getElementById(id);
   if(elem.cacheClassElem)
     elem.className = elem.cacheClassElem;
   if(elem.cacheClassTarget)
     target.className = elem.cacheClassTarget;
 }
/*]]>*///-->
</script>
<script type="text/javascript" src="/assets/MathJax/MathJax.js"></script>
<script type="text/javascript">
<!--/*--><![CDATA[/*><!--*/
    MathJax.Hub.Config({
        // Only one of the two following lines, depending on user settings
        // First allows browser-native MathML display, second forces HTML/CSS
        //  config: ["MMLorHTML.js"], jax: ["input/TeX"],
            jax: ["input/TeX", "output/HTML-CSS"],
        extensions: ["tex2jax.js","TeX/AMSmath.js","TeX/AMSsymbols.js",
                     "TeX/noUndefined.js"],
        tex2jax: {
            inlineMath: [ ["\\(","\\)"] ],
            displayMath: [ ['$$','$$'], ["\\[","\\]"], ["\\begin{displaymath}","\\end{displaymath}"] ],
            skipTags: ["script","noscript","style","textarea","pre","code"],
            ignoreClass: "tex2jax_ignore",
            processEscapes: false,
            processEnvironments: true,
            preview: "TeX"
        },
        showProcessingMessages: true,
        displayAlign: "center",
        displayIndent: "2em",

        "HTML-CSS": {
             scale: 100,
             availableFonts: ["STIX","TeX"],
             preferredFont: "TeX",
             webFont: "TeX",
             imageFont: "TeX",
             showMathMenu: true,
        },
        MMLorHTML: {
             prefer: {
                 MSIE:    "MML",
                 Firefox: "MML",
                 Opera:   "HTML",
                 other:   "HTML"
             }
        }
    });
/*]]>*///-->
</script>
</head>
<body>
<div id="content">
<h1 class="title">deep learning of binary hash codes for fast image retrieval</h1>
<div id="table-of-contents">
<h2>&#30446;&#24405;</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="#sec-1">Deep Learning of Binary Hash Codes for Fast Image Retrieval</a>
<ul>
<li><a href="#sec-1-1">abstract</a></li>
<li><a href="#sec-1-2">introduction</a></li>
<li><a href="#sec-1-3">related work</a></li>
<li><a href="#sec-1-4">method</a>
<ul>
<li><a href="#sec-1-4-1">Learning Hash-like Binary Codes</a></li>
<li><a href="#sec-1-4-2">Image Retrieval via Hierarchical Deep Search</a></li>
</ul>
</li>
<li><a href="#sec-1-5">Experimental Results</a>
<ul>
<li><a href="#sec-1-5-1">Datasets</a></li>
<li><a href="#sec-1-5-2">Ealuation Metrics</a></li>
<li><a href="#sec-1-5-3">Results on MNIST Dataset</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</div>
</div>



<div id="outline-container-sec-1" class="outline-2">
<h2 id="sec-1">Deep Learning of Binary Hash Codes for Fast Image Retrieval</h2>
<div class="outline-text-2" id="text-1">
</div><div id="outline-container-sec-1-1" class="outline-3">
<h3 id="sec-1-1">abstract</h3>
<div class="outline-text-3" id="text-1-1">
<p>
近似最近近邻搜索是大规模图像抽取的一个有效策略。本文基于CNN网络的最近研究，提
出了一个有效的深度学习框架生成二值码，用于快速的图像抽取。具体思想是，当图像
具有对应的标签数据，通过深度学习的隐层可以学习到二值码来表示数据的标签。本文
提出的方法不同于其他有监督的方法，需要提供pair-wised的数据来进行学习；本文提
出的学习方法只需要point-wised数据即可。
</p>
</div>
</div>


<div id="outline-container-sec-1-2" class="outline-3">
<h3 id="sec-1-2">introduction</h3>
<div class="outline-text-3" id="text-1-2">
<p>
<code>Content-Based Image Retrieval</code> 通过对图像内容进行分析，然后检索相似图像。因
此，图像内容的描述和相似度衡量便是关键。在深度学习以前，主要是通过人工构建图
像描述符来描述图像内容；深度学习爆发后，基于CNN的图像技术使得图像处理的相关技
术都得到的大量的提升。
</p>


<p>
得益于CNN网络能够学习丰富的中级图像描述符， =AlexNet=<sup><a id="fnr.1" name="fnr.1" class="footref" href="#fn.1">1</a></sup> 使用第7层的特征作为特征
描述符进行图像抽取，获取了较好的结果。然而由于直接在CNN高维特征（4096）进行相
似度计算是比较低效的，可以对特征进行 <code>PCA</code> 降维<sup><a id="fnr.2" name="fnr.2" class="footref" href="#fn.2">2</a></sup>。
</p>


<p>
<code>CBIR</code> 中图像描述和相似计算都是比较关键的。在大规模图像抽取中，由于面对的是高
维特征，常规的检索方式并不适合该场景。近似近邻(<code>Approximate Nearest
   Neighbor</code>)或者基于哈希的方法(<code>Hasing Based Method</code>)比较适合该场景。将高维特征
映射到低维空间，然后生成二值码，通过二值匹配或者Hamming距离计算，可以快速的实
现相似度计算。有些方法<sup><a id="fnr.3" name="fnr.3" class="footref" href="#fn.3">3</a></sup>都需要构建一个图片近似矩阵，来构建pair-wised的训练集，
从而在低维空间拉近相似图片的距离。
</p>


<p>
本文假设图片具有标签数据后，利用CNN直接学习图像的二值描述。本方法具体有如下特
性：
</p>
<ul class="org-ul">
<li>提出了一个简单有效的有监督学习框架，用来实现图像抽取
</li>
<li>简单的改动网络模型，就可以学习出图像的描述，并可以快速的实现图像抽取
</li>
<li>具有较好的性能，例如在CIFAR-10和MNIST数据集。
</li>
<li>训练数据不需要pair-wised，能够快速扩展到大规模数据集上
</li>
</ul>
</div>
</div>



<div id="outline-container-sec-1-3" class="outline-3">
<h3 id="sec-1-3">related work</h3>
<div class="outline-text-3" id="text-1-3">
<p>
哈希算法可以分为两大类：有监督和无监督方法。
</p>

<p>
无监督方法使用没有标记的数据学习哈希函数。 =Locality-Sensitive
Hashing(LSH)=<sup><a id="fnr.4" name="fnr.4" class="footref" href="#fn.4">4</a></sup> 通过最大化相似数据分到相似二值码的概率。 =Spectral
Hashing(SH)=<sup><a id="fnr.5" name="fnr.5" class="footref" href="#fn.5">5</a></sup> 通过对数据做PCA降维，在PCA主方向上取阈值截断获取二值码。
</p>

<p>
有监督方法可以更好地提升检索性能。有监督方法<sup><a id="fnr.6" name="fnr.6" class="footref" href="#fn.6">6</a></sup><sup>, </sup><sup><a id="fnr.7" name="fnr.7" class="footref" href="#fn.7">7</a></sup><sup>, </sup><sup><a id="fnr.8" name="fnr.8" class="footref" href="#fn.8">8</a></sup>引入标注数据，
并且使用pair-wised数据来学习哈希函数。
</p>

<p>
大多数深度结构用于哈希函数学习都是使用 <code>auto-encoder</code> 结构<sup><a id="fnr.9" name="fnr.9" class="footref" href="#fn.9">9</a></sup><sup>, </sup><sup><a id="fnr.10" name="fnr.10" class="footref" href="#fn.10">10</a></sup>。
</p>

<p>
方法<sup><a id="fnr.3.100" name="fnr.3.100" class="footref" href="#fn.3">3</a></sup>采取对相似矩阵进行矩阵分解，然后学习隐特征的方法，获取较好的效果，
但是受限于需要标注大量数据，无法很好的扩展到大规模数据集上。
</p>
</div>
</div>


<div id="outline-container-sec-1-4" class="outline-3">
<h3 id="sec-1-4">method</h3>
<div class="outline-text-3" id="text-1-4">
<p>
图示1为本文提出的深度学习框架。主要包含3部分：
</p>
<ol class="org-ol">
<li>预训练好的深度网络，本文采取的 <code>AlexNet</code>
</li>
<li>对隐特征进行微调，学习图像标签数据和哈希函数
</li>
<li>图像抽取功能
</li>
</ol>



<div class="figure">
<p><img src="assets/deep-learning-of-binary-hash-codes-for-fast-image-retrieval-1.png" alt="deep-learning-of-binary-hash-codes-for-fast-image-retrieval-1.png" />
</p>
<p><span class="figure-number">&#22270;1&nbsp;</span> 图示1</p>
</div>
</div>

<div id="outline-container-sec-1-4-1" class="outline-4">
<h4 id="sec-1-4-1">Learning Hash-like Binary Codes</h4>
<div class="outline-text-4" id="text-1-4-1">
<p>
很多文章都证明了 <code>Alexet</code> 的 F<sub>6-8</sub> 都可以作为图像的描述，直接使用这些高维特征进行图像检索，需要消耗大量的计算。
</p>

<p>
本文提出了基于图像标签进行图像描述学习和哈希函数获取二值码进行图像检索的方法。
假设分类层F<sub>8</sub>的结果依赖于 <code>Latent Layer</code> h个节点的 <code>on or off</code> 的状态。即相
似的二值码会推导出相同的标签。通过在F<sub>7</sub>和F<sub>8</sub>层之间引入一个隐层H，该隐层为一
个全链接层，该层节点状态由后续的分类层F<sub>8</sub>层根据分类结果决定，该层的节点的激
活函数为 <code>sigmoid</code> ，所以该节点的值在[0-1]之间。
</p>

<p>
CNN网络的权重值为预训练的AlexNet，隐层H和分类层F<sub>8</sub>随机初始化。整个网络通过反
向传播进行学习。
</p>
</div>
</div>

<div id="outline-container-sec-1-4-2" class="outline-4">
<h4 id="sec-1-4-2">Image Retrieval via Hierarchical Deep Search</h4>
<div class="outline-text-4" id="text-1-4-2">
<p>
=ZFNet=<sup><a id="fnr.11" name="fnr.11" class="footref" href="#fn.11">11</a></sup> 分析了深度CNN网络的浅层网络用来学习局部可视化描述符，深层网络
用来获取语义信息来识别。本文采取一个由粗到细的检索策略，用于快速和准确地进行
图像检索。首先采用H层的二值码进行召唤，然后采用最深层的中间图像描述进行排序。
</p>
</div>

<ul class="org-ul"><li><a id="sec-1-4-2-1" name="sec-1-4-2-1"></a>Coarse-level Search<br  /><div class="outline-text-5" id="text-1-4-2-1">
<p>
首先根据检索图像 <i>I</i> ，计算出H层的输出作为图像的签名(Signature)，表示为
<i>Out(H)</i> ，对激活值进行二值化。对于每一位 <i>j=1&#x2026;h</i> (<i>h</i> 为H层的节点数量)，
计算二值码：
</p>

\begin{equation}
H^j = \left\{
\begin{aligned}
1 & & Out^j(H) \geq 0.5 \\
0 & & otherwise.
\end{aligned}
\right.
\end{equation}

<p>
\(\Gamma=\{I_1,I_2,...,I_n\}\) 表示需要检索的候选图像集。对应的二值集合为
\(\Gamma_{H} = \{H_1,H_2,...,H_n\}\) ，其中 H_i &isin; {0,1}^h。给定一个输入图片，
根据二值码的Hamming距离小于一定的阈值获取候选集 <i>P</i> 。
</p>
</div>
</li>


<li><a id="sec-1-4-2-2" name="sec-1-4-2-2"></a>Fine-level Search<br  /><div class="outline-text-5" id="text-1-4-2-2">
<p>
获取F<sub>7</sub>层的特征作为描述，设定 V_q 和 V<sub>i</sub><sup>P</sup> 分别表示请求图像和候选集 <i>P</i> 中
图像的F<sub>7</sub>特征。采取 Euclidean 距离作为相似度度量函数：
</p>

\begin{equation} 
s_i = \Arrowvert V_q - V_{i}^{q}\Arrowvert
\end{equation}

<p>
Euclidean距离越小，相似度越高。
</p>
</div>
</li></ul>
</div>
</div>


<div id="outline-container-sec-1-5" class="outline-3">
<h3 id="sec-1-5">Experimental Results</h3>
<div class="outline-text-3" id="text-1-5">
<p>
首先介绍一下实验用的数据集（MNIST，CIFAR-10），然后和其他一些算法进行对比。最后会在 <code>Yahoo-1M</code> 数据集上进行验证。
</p>
</div>

<div id="outline-container-sec-1-5-1" class="outline-4">
<h4 id="sec-1-5-1">Datasets</h4>
<div class="outline-text-4" id="text-1-5-1">
<p>
<code>MNIST Dataset</code> 是一个具有10个类别的手写体数字识别数据集。60K的训练数据和
10000的测试数据。所有的图像都已经归一化成了28*28的灰度图像。
</p>


<div class="figure">
<p><img src="assets/deep-learning-of-binary-hash-codes-for-fast-image-retrieval-2.png" alt="deep-learning-of-binary-hash-codes-for-fast-image-retrieval-2.png" />
</p>
<p><span class="figure-number">&#22270;2&nbsp;</span> MNIST Dataset</p>
</div>

<p>
<code>CIFAR—10 Dataset</code> 是一个具有10个类别的，每个类别包括6k图像，总共具有60K图像
的数据集，训练和测试数据集分成50K和10K。
</p>


<div class="figure">
<p><img src="assets/deep-learning-of-binary-hash-codes-for-fast-image-retrieval-3.png" alt="deep-learning-of-binary-hash-codes-for-fast-image-retrieval-3.png" />
</p>
<p><span class="figure-number">&#22270;3&nbsp;</span> CIFAR-10 Dataset</p>
</div>

<p>
<code>Yahoo-1M Dataset</code> 包括大概1M的电商产品图像，分为了116类。
</p>


<div class="figure">
<p><img src="assets/deep-learning-of-binary-hash-codes-for-fast-image-retrieval-4.png" alt="deep-learning-of-binary-hash-codes-for-fast-image-retrieval-4.png" />
</p>
<p><span class="figure-number">&#22270;4&nbsp;</span> Yahoo-1M Dataset</p>
</div>


<p>
在数据集 MNIST 和 CIFAR-10 上使用了hash code和其他哈希算法进行了对比，在
Yahoo-1M上去评测的图像检索的效果。
</p>
</div>
</div>


<div id="outline-container-sec-1-5-2" class="outline-4">
<h4 id="sec-1-5-2">Ealuation Metrics</h4>
<div class="outline-text-4" id="text-1-5-2">
<p>
采取了给予排序的评测方法<sup><a id="fnr.12" name="fnr.12" class="footref" href="#fn.12">12</a></sup>，有一个请求图片 <i>q</i> 和一个相似度度量函数，每
一个图片都可以得到一个排序。对top-k的图片进行评测计算：
</p>

\begin{equation}
Precision@k = \frac{\sum_{i=1}^{k}Rel(i)}{k}
\end{equation}

<p>
其中： <i>Rel(i)</i> 为请求图片和第 <i>i-th</i> 图片的相似度。这里仅仅考虑是否是同一个
标签图片， <i>Rel(i)</i> &isin; {0, 1}。
</p>
</div>
</div>

<div id="outline-container-sec-1-5-3" class="outline-4">
<h4 id="sec-1-5-3">Results on MNIST Dataset</h4>
<div class="outline-text-4" id="text-1-5-3">
</div><ul class="org-ul"><li><a id="sec-1-5-3-1" name="sec-1-5-3-1"></a>Performance of Image Classification<br  /><div class="outline-text-5" id="text-1-5-3-1">
<p>
F<sub>8</sub> 设置为10-way softmax去预测10个数字类别。为了评估隐层H，设置H层的节点
数 <i>h</i> 为48和128。采取SGD训练CNN，50000迭代次数，0.001学习率。
</p>
</div>
</li></ul>
</div>
</div>
</div>
<div id="footnotes">
<h2 class="footnotes">&#33050;&#27880;: </h2>
<div id="text-footnotes">

<div class="footdef"><sup><a id="fn.1" name="fn.1" class="footnum" href="#fnr.1">1</a></sup> <p class="footpara">
ImageNet classification with deep convolutional neural networks
</p></div>

<div class="footdef"><sup><a id="fn.2" name="fn.2" class="footnum" href="#fnr.2">2</a></sup> <p class="footpara">
Neural codes for image retrieval. In Proc. ECCV, pages 584–599. Springer, 2014
</p></div>

<div class="footdef"><sup><a id="fn.3" name="fn.3" class="footnum" href="#fnr.3">3</a></sup> <p class="footpara">
Supervised hashing for image retrieval via image representation learning.
In Proc. AAAI, 2014
</p></div>

<div class="footdef"><sup><a id="fn.4" name="fn.4" class="footnum" href="#fnr.4">4</a></sup> <p class="footpara">
Similarity search in high dimensions via hashing. In VLDB, volume 99,
pages 518–529, 1999
</p></div>

<div class="footdef"><sup><a id="fn.5" name="fn.5" class="footnum" href="#fnr.5">5</a></sup> <p class="footpara">
Spectral hashing. In Proc. NIPS, pages 1753–1760, 2009.
</p></div>

<div class="footdef"><sup><a id="fn.6" name="fn.6" class="footnum" href="#fnr.6">6</a></sup> <p class="footpara">
Supervised hashing with kernels. In Proc. CVPR, pages 2074–2081, 2012
</p></div>

<div class="footdef"><sup><a id="fn.7" name="fn.7" class="footnum" href="#fnr.7">7</a></sup> <p class="footpara">
Minimal loss hashing for compact binary codes. In Proc. ICML, pages
353–360, 2011
</p></div>

<div class="footdef"><sup><a id="fn.8" name="fn.8" class="footnum" href="#fnr.8">8</a></sup> <p class="footpara">
Learning to hash with binary reconstructive embeddings.In Proc. NIPS,
pages 1042–1050,2009.
</p></div>

<div class="footdef"><sup><a id="fn.9" name="fn.9" class="footnum" href="#fnr.9">9</a></sup> <p class="footpara">
Semantic hashing. International Journal of Approximate Reasoning, 500(3):500, 2007.
</p></div>

<div class="footdef"><sup><a id="fn.10" name="fn.10" class="footnum" href="#fnr.10">10</a></sup> <p class="footpara">
Using very deep autoencoders for content-based image retrieval.In
ESANN, 2011.
</p></div>

<div class="footdef"><sup><a id="fn.11" name="fn.11" class="footnum" href="#fnr.11">11</a></sup> <p class="footpara">
Visualizing and understanding convolutional networks. In Proc. ECCV,
pages 818–833. Springer, 2014.
</p></div>

<div class="footdef"><sup><a id="fn.12" name="fn.12" class="footnum" href="#fnr.12">12</a></sup> <p class="footpara">
Hierarchical semantic indexing for large scale image retrieval. In Proc.
CVPR, 2011.
</p></div>


</div>
</div></div>
</body>
</html>
