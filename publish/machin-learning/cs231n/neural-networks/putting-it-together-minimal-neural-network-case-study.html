<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="zh-CN" xml:lang="zh-CN">
<head>
<title>Putting it together: Minimal Neural Network Case Study</title>
<!-- 2018-08-27 Mon 09:39 -->
<meta  http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta  name="generator" content="Org-mode" />
<meta  name="author" content="stupid-coder" />
<link rel="stylesheet" title="Standard" href="/assets/worg.css" type="text/css" />
           <link rel="alternate stylesheet" title="Zenburn" href="/assets/worg-zenburn.css" type="text/css" />
           <link rel="alternate stylesheet" title="Classic" href="/assets/worg-classic.css" type="text/css" />
<script type="text/javascript">
/*
@licstart  The following is the entire license notice for the
JavaScript code in this tag.

Copyright (C) 2012-2013 Free Software Foundation, Inc.

The JavaScript code in this tag is free software: you can
redistribute it and/or modify it under the terms of the GNU
General Public License (GNU GPL) as published by the Free Software
Foundation, either version 3 of the License, or (at your option)
any later version.  The code is distributed WITHOUT ANY WARRANTY;
without even the implied warranty of MERCHANTABILITY or FITNESS
FOR A PARTICULAR PURPOSE.  See the GNU GPL for more details.

As additional permission under GNU GPL version 3 section 7, you
may distribute non-source (e.g., minimized or compacted) forms of
that code without the copy of the GNU GPL normally required by
section 4, provided you include this license notice and a URL
through which recipients can access the Corresponding Source.


@licend  The above is the entire license notice
for the JavaScript code in this tag.
*/
<!--/*--><![CDATA[/*><!--*/
 function CodeHighlightOn(elem, id)
 {
   var target = document.getElementById(id);
   if(null != target) {
     elem.cacheClassElem = elem.className;
     elem.cacheClassTarget = target.className;
     target.className = "code-highlighted";
     elem.className   = "code-highlighted";
   }
 }
 function CodeHighlightOff(elem, id)
 {
   var target = document.getElementById(id);
   if(elem.cacheClassElem)
     elem.className = elem.cacheClassElem;
   if(elem.cacheClassTarget)
     target.className = elem.cacheClassTarget;
 }
/*]]>*///-->
</script>
<script type="text/javascript" src="/assets/MathJax/MathJax.js"></script>
<script type="text/javascript">
<!--/*--><![CDATA[/*><!--*/
    MathJax.Hub.Config({
        // Only one of the two following lines, depending on user settings
        // First allows browser-native MathML display, second forces HTML/CSS
        //  config: ["MMLorHTML.js"], jax: ["input/TeX"],
            jax: ["input/TeX", "output/HTML-CSS"],
        extensions: ["tex2jax.js","TeX/AMSmath.js","TeX/AMSsymbols.js",
                     "TeX/noUndefined.js"],
        tex2jax: {
            inlineMath: [ ["\\(","\\)"] ],
            displayMath: [ ['$$','$$'], ["\\[","\\]"], ["\\begin{displaymath}","\\end{displaymath}"] ],
            skipTags: ["script","noscript","style","textarea","pre","code"],
            ignoreClass: "tex2jax_ignore",
            processEscapes: false,
            processEnvironments: true,
            preview: "TeX"
        },
        showProcessingMessages: true,
        displayAlign: "center",
        displayIndent: "2em",

        "HTML-CSS": {
             scale: 100,
             availableFonts: ["STIX","TeX"],
             preferredFont: "TeX",
             webFont: "TeX",
             imageFont: "TeX",
             showMathMenu: true,
        },
        MMLorHTML: {
             prefer: {
                 MSIE:    "MML",
                 Firefox: "MML",
                 Opera:   "HTML",
                 other:   "HTML"
             }
        }
    });
/*]]>*///-->
</script>
</head>
<body>
<div id="content">
<h1 class="title">Putting it together: Minimal Neural Network Case Study</h1>
<div id="table-of-contents">
<h2>&#30446;&#24405;</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="#sec-1">Generating some data</a></li>
<li><a href="#sec-2">Training a Softmax Linear Classifier</a>
<ul>
<li><a href="#sec-2-1">Initialize the parameters</a></li>
<li><a href="#sec-2-2">Compute the class scores</a></li>
<li><a href="#sec-2-3">Compute the loss</a></li>
<li><a href="#sec-2-4">Computing the analytic gradient with backpropagation</a></li>
<li><a href="#sec-2-5">Performing a parameter update</a></li>
<li><a href="#sec-2-6">Putting it all together: Training a Softmax Classifier</a></li>
</ul>
</li>
<li><a href="#sec-3">Training a Neural Network</a></li>
</ul>
</div>
</div>
<p>
<a href="http://cs231n.github.io/neural-networks-case-study/">cs231n-neural-networks-case-study</a>
</p>

<p>
本文将实现一个完整的，具有教学意义的神经网络。首先实现一个简单的线性分类器，然后
在该分类器的基础上稍作改动就可以扩展成 2-层神经网络。
</p>

<div id="outline-container-sec-1" class="outline-2">
<h2 id="sec-1">Generating some data</h2>
<div class="outline-text-2" id="text-1">
<p>
首先生成一个分类数据集，该数据集不需要是线性可分的。一个比较适合的数据集是螺旋分
布：
</p>
<div class="org-src-container">

<pre class="src src-python"><span style="font-weight: bold; font-style: italic;">N</span> = 100 <span style="font-weight: bold; font-style: italic;"># </span><span style="font-weight: bold; font-style: italic;">number of points per class</span>
<span style="font-weight: bold; font-style: italic;">D</span> = 2 <span style="font-weight: bold; font-style: italic;"># </span><span style="font-weight: bold; font-style: italic;">dimensionality</span>
<span style="font-weight: bold; font-style: italic;">K</span> = 3 <span style="font-weight: bold; font-style: italic;"># </span><span style="font-weight: bold; font-style: italic;">number of classes</span>
<span style="font-weight: bold; font-style: italic;">X</span> = np.zeros((N*K,D)) <span style="font-weight: bold; font-style: italic;"># </span><span style="font-weight: bold; font-style: italic;">data matrix (each row = single example)</span>
<span style="font-weight: bold; font-style: italic;">y</span> = np.zeros(N*K, dtype=<span style="color: #daa520; font-style: italic;">'uint8'</span>) <span style="font-weight: bold; font-style: italic;"># </span><span style="font-weight: bold; font-style: italic;">class labels</span>
<span style="color: #00bfff; font-weight: bold;">for</span> j <span style="color: #00bfff; font-weight: bold;">in</span> <span style="font-weight: bold;">xrange</span>(K):
  <span style="font-weight: bold; font-style: italic;">ix</span> = <span style="font-weight: bold;">range</span>(N*j,N*(j+1))
  <span style="font-weight: bold; font-style: italic;">r</span> = np.linspace(0.0,1,N) <span style="font-weight: bold; font-style: italic;"># </span><span style="font-weight: bold; font-style: italic;">radius</span>
  <span style="font-weight: bold; font-style: italic;">t</span> = np.linspace(j*4,(j+1)*4,N) + np.random.randn(N)*0.2 <span style="font-weight: bold; font-style: italic;"># </span><span style="font-weight: bold; font-style: italic;">theta</span>
  <span style="font-weight: bold; font-style: italic;">X</span>[ix] = np.c_[r*np.sin(t), r*np.cos(t)]
  <span style="font-weight: bold; font-style: italic;">y</span>[ix] = j
  <span style="font-weight: bold; font-style: italic;"># </span><span style="font-weight: bold; font-style: italic;">lets visualize the data:</span>
plt.scatter(X[:, 0], X[:, 1], c=y, s=40, cmap=plt.cm.Spectral)
plt.show()
</pre>
</div>
<hr  />
<div class="center">

<div class="figure">
<p><img src="assets/eg/spiral_raw.png" alt="spiral_raw.png" />
</p>
<p><span class="figure-number">&#22270;1&nbsp;</span> 螺旋数据集</p>
</div>
<blockquote>
<p>
具有三种类别信息的螺旋数据集。
</p>
</blockquote>
</div>
<hr  />
<p>
一般需要对输入的数据进行预处理，包括中心化，标准差，PCA 等。但是本样本集已经是分
布在[-1, +1]之间，且样本维度较小，所以无需处理。
</p>
</div>
</div>

<div id="outline-container-sec-2" class="outline-2">
<h2 id="sec-2">Training a Softmax Linear Classifier</h2>
<div class="outline-text-2" id="text-2">
</div><div id="outline-container-sec-2-1" class="outline-3">
<h3 id="sec-2-1">Initialize the parameters</h3>
<div class="outline-text-3" id="text-2-1">
<p>
首先训练一个 <i>Softmax classifier</i> 线性得分函数，并使用交叉熵作为损失函数。线性分
类器由权值矩阵 <i>W</i> 和 偏置向量 <i>b</i> 组成。首先初始化参数：
</p>
<div class="org-src-container">

<pre class="src src-python"><span style="font-weight: bold; font-style: italic;"># </span><span style="font-weight: bold; font-style: italic;">initialize parameters randomly</span>
<span style="font-weight: bold; font-style: italic;">W</span> = 0.01 * np.random.randn(D,K)
<span style="font-weight: bold; font-style: italic;">b</span> = np.zeros((1,K))
</pre>
</div>
</div>
</div>

<div id="outline-container-sec-2-2" class="outline-3">
<h3 id="sec-2-2">Compute the class scores</h3>
<div class="outline-text-3" id="text-2-2">
<p>
通过矩阵运算可以并行计算样本的所有类别的对应得分：
</p>
<div class="org-src-container">

<pre class="src src-python"><span style="font-weight: bold; font-style: italic;"># </span><span style="font-weight: bold; font-style: italic;">compute class scores for a linear classifier</span>
<span style="font-weight: bold; font-style: italic;">scores</span> = np.dot(X, W) + b
</pre>
</div>

<p>
上述的样本集具有 300 个 2-D 样本，乘法运算后 <i>scores</i> 数组为[300,3]维度，每一行
为单个样本所有对应的 3 个类别的对应得分(蓝色，红色，黄色)。
</p>
</div>
</div>

<div id="outline-container-sec-2-3" class="outline-3">
<h3 id="sec-2-3">Compute the loss</h3>
<div class="outline-text-3" id="text-2-3">
<p>
对于得分函数，希望正确类别的得分能够高于其他类别的得分。可以通过定义的损失函数来
度量得分函数的对应类别的得分是否高于其他类别得分，从而能够指导优化算法进行学习。
本例子，采取 <i>Softmax classifier</i> 对应的交叉熵损失函数(<i>cross-entropy loss</i>) 。
\(f\) 计算结果为类别得分向量(3 维向量)，单样本损失函数为：
$$L_i = -\log\left(\frac{e^{f_{y_i}}}{ \sum_j e^{f_j} }\right)$$
</p>

<p>
可以看到， <i>Softmax classifier</i> 将得分看作是类别的未归一概化对数率。对得分求解指
数获取未归一化概率，然后通过归一化得到概率。所以在 log 函数中的为正确类别的归一化
概率。当正确类别的概率接近 0，那么损失接近正无穷；相反，如果正确类别的概率接近 1，
那么损失接近 0。
</p>

<p>
全部 <i>Softmax classifier</i> 损失值由两部分组成：交叉熵的数据损失和正则损失：
$$L =  \underbrace{ \frac{1}{N} \sum_i L_i }_\text{data loss} + \underbrace{
\frac{1}{2} \lambda \sum_k\sum_l W_{k,l}^2 }_\text{regularization loss} \\\\$$
</p>

<p>
一旦获取了样本的得分，那么就可以直接通过上述公式计算损失值。第一步，根据得分向量，
计算概率：
</p>
<div class="org-src-container">

<pre class="src src-python"><span style="font-weight: bold; font-style: italic;">num_examples</span> = X.shape[0]
<span style="font-weight: bold; font-style: italic;"># </span><span style="font-weight: bold; font-style: italic;">get unnormalized probabilities</span>
<span style="font-weight: bold; font-style: italic;">exp_scores</span> = np.exp(scores)
<span style="font-weight: bold; font-style: italic;"># </span><span style="font-weight: bold; font-style: italic;">normalize them for each example</span>
<span style="font-weight: bold; font-style: italic;">probs</span> = exp_scores / np.<span style="font-weight: bold;">sum</span>(exp_scores, axis=1, keepdims=<span style="font-weight: bold; text-decoration: underline;">True</span>)
</pre>
</div>

<p>
<i>probs</i> 是[300,3]的概率数组，每一行是单个样本对于三个类别的概率。则可以计算对应
的概率对数作为损失：
</p>
<div class="org-src-container">

<pre class="src src-python"><span style="font-weight: bold; font-style: italic;">correct_logprobs</span> = -np.log(probs[<span style="font-weight: bold;">range</span>(num_examples),y])
</pre>
</div>

<p>
整体损失为对数概率的平均加上正则损失：
</p>
<div class="org-src-container">

<pre class="src src-python"><span style="font-weight: bold; font-style: italic;"># </span><span style="font-weight: bold; font-style: italic;">compute the loss: average cross-entropy loss and regularization</span>
<span style="font-weight: bold; font-style: italic;">data_loss</span> = np.<span style="font-weight: bold;">sum</span>(correct_logprobs)/num_examples
<span style="font-weight: bold; font-style: italic;">reg_loss</span> = 0.5*reg*np.<span style="font-weight: bold;">sum</span>(W*W)
<span style="font-weight: bold; font-style: italic;">loss</span> = data_loss + reg_loss
</pre>
</div>

<p>
正则系数 \(\lambda\) 就是 <i>reg</i> 。开始基于随机初始化参数的网络的损失接近 <i>loss =
1.1</i> ，是因为 <i>np.log(1.0/3) = -1.1</i> ，因为随机初始化的网络计算的类别概率接近 1.0/3。
</p>
</div>
</div>

<div id="outline-container-sec-2-4" class="outline-3">
<h3 id="sec-2-4">Computing the analytic gradient with backpropagation</h3>
<div class="outline-text-3" id="text-2-4">
<p>
那么只需要最小化损失函数，就可以找到更好的分类器。主要是通过梯度下降的方法来最小
化损失函数。通过计算损失函数对参数的梯度，并朝着梯度的反方向(梯度的方向为函数上
升的方向)迭代，使得整个损失函数变小。引入中间变量 \(p\) ，作为样本经过分类器计算的
归一化概率向量。样本的损失函数：
$$p_k = \frac{e^{f_k}}{ \sum_j e^{f_j} } \hspace{1in} L_i
=-\log\left(p_{y_i}\right)$$
</p>

<p>
通过求解梯度 \(\partial L_i / \partial f_k\) ，可以获得整体损失下降的方向。应用链
式法则可以较为容易的计算出梯度值：
$$\frac{\partial L_i }{ \partial f_k } = p_k - \mathbb{1}(y_i = k)$$
</p>

<div class="org-src-container">

<pre class="src src-python"><span style="font-weight: bold; font-style: italic;">dscores</span> = probs
<span style="font-weight: bold; font-style: italic;">dscores</span>[<span style="font-weight: bold;">range</span>(num_examples),y] -= 1
<span style="font-weight: bold; font-style: italic;">dscores</span> /= num_examples
</pre>
</div>

<p>
组后，我们知道 <i>scores = np.dot(X,W)+b</i> ，可以将梯度反向传播给参数 <i>W</i> 和 <i>b</i> :
</p>
<div class="org-src-container">

<pre class="src src-python"><span style="font-weight: bold; font-style: italic;">dW</span> = np.dot(X.T, dscores)
<span style="font-weight: bold; font-style: italic;">db</span> = np.<span style="font-weight: bold;">sum</span>(dscores, axis=0, keepdims=<span style="font-weight: bold; text-decoration: underline;">True</span>)
<span style="font-weight: bold; font-style: italic;">dW</span> += reg*W <span style="font-weight: bold; font-style: italic;"># </span><span style="font-weight: bold; font-style: italic;">don't forget the regularization gradient</span>
</pre>
</div>

<p>
可以看到最后一行是正则项的梯度。正则梯度具有很简单的计算形式 <i>reg*W</i> ，因为采取
0.5 作为正则项的一个前置系数， \(\frac{d}{dw} ( \frac{1}{2} \lambda w^2) = \lambda
w\) 。
</p>
</div>
</div>

<div id="outline-container-sec-2-5" class="outline-3">
<h3 id="sec-2-5">Performing a parameter update</h3>
<div class="outline-text-3" id="text-2-5">
<p>
通过计算损失函数的梯度，我们知道了参数对损失函数的影响，根据梯度就可以执行参数反
方向更新，从而实现损失函数的下降：
</p>
<div class="org-src-container">

<pre class="src src-python"><span style="font-weight: bold; font-style: italic;"># </span><span style="font-weight: bold; font-style: italic;">perform a parameter update</span>
<span style="font-weight: bold; font-style: italic;">W</span> += -step_size * dW
<span style="font-weight: bold; font-style: italic;">b</span> += -step_size * db
</pre>
</div>
</div>
</div>

<div id="outline-container-sec-2-6" class="outline-3">
<h3 id="sec-2-6">Putting it all together: Training a Softmax Classifier</h3>
<div class="outline-text-3" id="text-2-6">
<p>
将上述各个部分放在一起，就是一个 <i>Softmax classifier</i> ：
</p>
<div class="org-src-container">

<pre class="src src-python"><span style="font-weight: bold; font-style: italic;">#</span><span style="font-weight: bold; font-style: italic;">Train a Linear Classifier</span>

<span style="font-weight: bold; font-style: italic;"># </span><span style="font-weight: bold; font-style: italic;">initialize parameters randomly</span>
<span style="font-weight: bold; font-style: italic;">W</span> = 0.01 * np.random.randn(D,K)
<span style="font-weight: bold; font-style: italic;">b</span> = np.zeros((1,K))

<span style="font-weight: bold; font-style: italic;"># </span><span style="font-weight: bold; font-style: italic;">some hyperparameters</span>
<span style="font-weight: bold; font-style: italic;">step_size</span> = 1e-0
<span style="font-weight: bold; font-style: italic;">reg</span> = 1e-3 <span style="font-weight: bold; font-style: italic;"># </span><span style="font-weight: bold; font-style: italic;">regularization strength</span>

<span style="font-weight: bold; font-style: italic;"># </span><span style="font-weight: bold; font-style: italic;">gradient descent loop</span>
<span style="font-weight: bold; font-style: italic;">num_examples</span> = X.shape[0]
<span style="color: #00bfff; font-weight: bold;">for</span> i <span style="color: #00bfff; font-weight: bold;">in</span> <span style="font-weight: bold;">xrange</span>(200):

  <span style="font-weight: bold; font-style: italic;"># </span><span style="font-weight: bold; font-style: italic;">evaluate class scores, [N x K]</span>
  <span style="font-weight: bold; font-style: italic;">scores</span> = np.dot(X, W) + b 

  <span style="font-weight: bold; font-style: italic;"># </span><span style="font-weight: bold; font-style: italic;">compute the class probabilities</span>
  <span style="font-weight: bold; font-style: italic;">exp_scores</span> = np.exp(scores)
  <span style="font-weight: bold; font-style: italic;">probs</span> = exp_scores / np.<span style="font-weight: bold;">sum</span>(exp_scores, axis=1, keepdims=<span style="font-weight: bold; text-decoration: underline;">True</span>) <span style="font-weight: bold; font-style: italic;"># </span><span style="font-weight: bold; font-style: italic;">[N x K]</span>

  <span style="font-weight: bold; font-style: italic;"># </span><span style="font-weight: bold; font-style: italic;">compute the loss: average cross-entropy loss and regularization</span>
  <span style="font-weight: bold; font-style: italic;">correct_logprobs</span> = -np.log(probs[<span style="font-weight: bold;">range</span>(num_examples),y])
  <span style="font-weight: bold; font-style: italic;">data_loss</span> = np.<span style="font-weight: bold;">sum</span>(correct_logprobs)/num_examples
  <span style="font-weight: bold; font-style: italic;">reg_loss</span> = 0.5*reg*np.<span style="font-weight: bold;">sum</span>(W*W)
  <span style="font-weight: bold; font-style: italic;">loss</span> = data_loss + reg_loss
  <span style="color: #00bfff; font-weight: bold;">if</span> i % 10 == 0:
    <span style="color: #00bfff; font-weight: bold;">print</span> <span style="color: #daa520; font-style: italic;">"iteration %d: loss %f"</span> % (i, loss)

  <span style="font-weight: bold; font-style: italic;"># </span><span style="font-weight: bold; font-style: italic;">compute the gradient on scores</span>
  <span style="font-weight: bold; font-style: italic;">dscores</span> = probs
  <span style="font-weight: bold; font-style: italic;">dscores</span>[<span style="font-weight: bold;">range</span>(num_examples),y] -= 1
  <span style="font-weight: bold; font-style: italic;">dscores</span> /= num_examples

  <span style="font-weight: bold; font-style: italic;"># </span><span style="font-weight: bold; font-style: italic;">backpropate the gradient to the parameters (W,b)</span>
  <span style="font-weight: bold; font-style: italic;">dW</span> = np.dot(X.T, dscores)
  <span style="font-weight: bold; font-style: italic;">db</span> = np.<span style="font-weight: bold;">sum</span>(dscores, axis=0, keepdims=<span style="font-weight: bold; text-decoration: underline;">True</span>)

  <span style="font-weight: bold; font-style: italic;">dW</span> += reg*W <span style="font-weight: bold; font-style: italic;"># </span><span style="font-weight: bold; font-style: italic;">regularization gradient</span>

  <span style="font-weight: bold; font-style: italic;"># </span><span style="font-weight: bold; font-style: italic;">perform a parameter update</span>
  <span style="font-weight: bold; font-style: italic;">W</span> += -step_size * dW
  <span style="font-weight: bold; font-style: italic;">b</span> += -step_size * db
</pre>
</div>

<p>
然后可以使用如下的代码评估训练集的准确性：
</p>
<div class="org-src-container">

<pre class="src src-python"><span style="font-weight: bold; font-style: italic;"># </span><span style="font-weight: bold; font-style: italic;">evaluate training set accuracy</span>
<span style="font-weight: bold; font-style: italic;">scores</span> = np.dot(X, W) + b
<span style="font-weight: bold; font-style: italic;">predicted_class</span> = np.argmax(scores, axis=1)
<span style="color: #00bfff; font-weight: bold;">print</span> <span style="color: #daa520; font-style: italic;">'training accuracy: %.2f'</span> % (np.mean(predicted_class == y))
</pre>
</div>

<p>
应该有 49% 的准确性，并不是特别好，因为该数据集不是线性可分的。可以将学习到的分
类平面画出来：
</p>
<hr  />
<div class="center">

<div class="figure">
<p><img src="assets/eg/spiral_linear.png" alt="spiral_linear.png" />
</p>
<p><span class="figure-number">&#22270;2&nbsp;</span> 线性分类器在螺旋数据集上的分类平面</p>
</div>
</div>
<hr  />
</div>
</div>
</div>

<div id="outline-container-sec-3" class="outline-2">
<h2 id="sec-3">Training a Neural Network</h2>
<div class="outline-text-2" id="text-3">
<p>
显然，线性分类器在不可线性分类的数据集并不实用。神经网络可以通过增加一层隐层来拟
合不可线性分类的数据集。需要套权值和偏置向量：
</p>
<div class="org-src-container">

<pre class="src src-python"><span style="font-weight: bold; font-style: italic;"># </span><span style="font-weight: bold; font-style: italic;">initialize parameters randomly</span>
<span style="font-weight: bold; font-style: italic;">h</span> = 100 <span style="font-weight: bold; font-style: italic;"># </span><span style="font-weight: bold; font-style: italic;">size of hidden layer</span>
<span style="font-weight: bold; font-style: italic;">W</span> = 0.01 * np.random.randn(D,h)
<span style="font-weight: bold; font-style: italic;">b</span> = np.zeros((1,h))
<span style="font-weight: bold; font-style: italic;">W2</span> = 0.01 * np.random.randn(h,K)
<span style="font-weight: bold; font-style: italic;">b2</span> = np.zeros((1,K))
</pre>
</div>

<p>
向前传播，计算得分：
</p>
<div class="org-src-container">

<pre class="src src-python"><span style="font-weight: bold; font-style: italic;"># </span><span style="font-weight: bold; font-style: italic;">evaluate class scores with a 2-layer Neural Network</span>
<span style="font-weight: bold; font-style: italic;">hidden_layer</span> = np.maximum(0, np.dot(X, W) + b) <span style="font-weight: bold; font-style: italic;"># </span><span style="font-weight: bold; font-style: italic;">note, ReLU activation</span>
<span style="font-weight: bold; font-style: italic;">scores</span> = np.dot(hidden_layer, W2) + b2
</pre>
</div>

<p>
可以看到只是增加了一行代码用来计算隐层状态，然后根据隐层计算输出层结果。最重要的
是，增加了非线性变换操作，实现了一个简单的 <i>ReLU</i> 。
</p>

<p>
基于得分的损失函数计算不需要改变。并且计算的损失函数针对得分的梯度 <i>dscores</i> 也
不需要改变。只是梯度反向传播是需要一定的修改。首先，执行隐层的反向传播。这一部分
看起来和前面的线性分类器的反向传播相似，除了将输入样本的 <i>X</i> 换成隐层的变量
<i>hidden_layer</i> ：
</p>
<div class="org-src-container">

<pre class="src src-python"><span style="font-weight: bold; font-style: italic;"># </span><span style="font-weight: bold; font-style: italic;">backpropate the gradient to the parameters</span>
<span style="font-weight: bold; font-style: italic;"># </span><span style="font-weight: bold; font-style: italic;">first backprop into parameters W2 and b2</span>
<span style="font-weight: bold; font-style: italic;">dW2</span> = np.dot(hidden_layer.T, dscores)
<span style="font-weight: bold; font-style: italic;">db2</span> = np.<span style="font-weight: bold;">sum</span>(dscores, axis=0, keepdims=<span style="font-weight: bold; text-decoration: underline;">True</span>)
</pre>
</div>

<p>
然后，由于 <i>hidden_layer</i> 本身也是其他层参数的函数，需要继续执行反向传播：
</p>
<div class="org-src-container">

<pre class="src src-python"><span style="font-weight: bold; font-style: italic;">dhidden</span> = np.dot(dscores, W2.T)
</pre>
</div>

<p>
现在知道了隐层输出的梯度，下一步是执行针对 <i>ReLU</i> 非线性变换的反向传播。这一步降
较为简单，主要原因是 <i>ReLU</i> 可以看作是传播的开发。 \(r = max(0, x)\) ，所以
\(\frac{dr}{dx} = 1(x > 0)\) 。应用链式法则，可以看到 <i>ReLU unit</i> 在输入大于 0 的时
候，允许梯度传播过去，否则就将梯度置为 0。简单的实现：
</p>
<div class="org-src-container">

<pre class="src src-python"><span style="font-weight: bold; font-style: italic;"># </span><span style="font-weight: bold; font-style: italic;">backprop the ReLU non-linearity</span>
<span style="font-weight: bold; font-style: italic;">dhidden</span>[hidden_layer &lt;= 0] = 0
</pre>
</div>

<p>
然后可以计算出第一层的参数的梯度：
</p>
<div class="org-src-container">

<pre class="src src-python"><span style="font-weight: bold; font-style: italic;"># </span><span style="font-weight: bold; font-style: italic;">finally into W,b</span>
<span style="font-weight: bold; font-style: italic;">dW</span> = np.dot(X.T, dhidden)
<span style="font-weight: bold; font-style: italic;">db</span> = np.<span style="font-weight: bold;">sum</span>(dhidden, axis=0, keepdims=<span style="font-weight: bold; text-decoration: underline;">True</span>)
</pre>
</div>

<p>
至此，我们就计算完成了一个 2 层的神经网络的梯度值 <i>dW,db,dW2,db2</i> 。整个代码：
</p>
<div class="org-src-container">

<pre class="src src-python"><span style="font-weight: bold; font-style: italic;"># </span><span style="font-weight: bold; font-style: italic;">initialize parameters randomly</span>
<span style="font-weight: bold; font-style: italic;">h</span> = 100 <span style="font-weight: bold; font-style: italic;"># </span><span style="font-weight: bold; font-style: italic;">size of hidden layer</span>
<span style="font-weight: bold; font-style: italic;">W</span> = 0.01 * np.random.randn(D,h)
<span style="font-weight: bold; font-style: italic;">b</span> = np.zeros((1,h))
<span style="font-weight: bold; font-style: italic;">W2</span> = 0.01 * np.random.randn(h,K)
<span style="font-weight: bold; font-style: italic;">b2</span> = np.zeros((1,K))

<span style="font-weight: bold; font-style: italic;"># </span><span style="font-weight: bold; font-style: italic;">some hyperparameters</span>
<span style="font-weight: bold; font-style: italic;">step_size</span> = 1e-0
<span style="font-weight: bold; font-style: italic;">reg</span> = 1e-3 <span style="font-weight: bold; font-style: italic;"># </span><span style="font-weight: bold; font-style: italic;">regularization strength</span>

<span style="font-weight: bold; font-style: italic;"># </span><span style="font-weight: bold; font-style: italic;">gradient descent loop</span>
<span style="font-weight: bold; font-style: italic;">num_examples</span> = X.shape[0]
<span style="color: #00bfff; font-weight: bold;">for</span> i <span style="color: #00bfff; font-weight: bold;">in</span> <span style="font-weight: bold;">xrange</span>(10000):

  <span style="font-weight: bold; font-style: italic;"># </span><span style="font-weight: bold; font-style: italic;">evaluate class scores, [N x K]</span>
  <span style="font-weight: bold; font-style: italic;">hidden_layer</span> = np.maximum(0, np.dot(X, W) + b) <span style="font-weight: bold; font-style: italic;"># </span><span style="font-weight: bold; font-style: italic;">note, ReLU activation</span>
  <span style="font-weight: bold; font-style: italic;">scores</span> = np.dot(hidden_layer, W2) + b2

  <span style="font-weight: bold; font-style: italic;"># </span><span style="font-weight: bold; font-style: italic;">compute the class probabilities</span>
  <span style="font-weight: bold; font-style: italic;">exp_scores</span> = np.exp(scores)
  <span style="font-weight: bold; font-style: italic;">probs</span> = exp_scores / np.<span style="font-weight: bold;">sum</span>(exp_scores, axis=1, keepdims=<span style="font-weight: bold; text-decoration: underline;">True</span>) <span style="font-weight: bold; font-style: italic;"># </span><span style="font-weight: bold; font-style: italic;">[N x K]</span>

  <span style="font-weight: bold; font-style: italic;"># </span><span style="font-weight: bold; font-style: italic;">compute the loss: average cross-entropy loss and regularization</span>
  <span style="font-weight: bold; font-style: italic;">correct_logprobs</span> = -np.log(probs[<span style="font-weight: bold;">range</span>(num_examples),y])
  <span style="font-weight: bold; font-style: italic;">data_loss</span> = np.<span style="font-weight: bold;">sum</span>(correct_logprobs)/num_examples
  <span style="font-weight: bold; font-style: italic;">reg_loss</span> = 0.5*reg*np.<span style="font-weight: bold;">sum</span>(W*W) + 0.5*reg*np.<span style="font-weight: bold;">sum</span>(W2*W2)
  <span style="font-weight: bold; font-style: italic;">loss</span> = data_loss + reg_loss
  <span style="color: #00bfff; font-weight: bold;">if</span> i % 1000 == 0:
    <span style="color: #00bfff; font-weight: bold;">print</span> <span style="color: #daa520; font-style: italic;">"iteration %d: loss %f"</span> % (i, loss)

  <span style="font-weight: bold; font-style: italic;"># </span><span style="font-weight: bold; font-style: italic;">compute the gradient on scores</span>
  <span style="font-weight: bold; font-style: italic;">dscores</span> = probs
  <span style="font-weight: bold; font-style: italic;">dscores</span>[<span style="font-weight: bold;">range</span>(num_examples),y] -= 1
  <span style="font-weight: bold; font-style: italic;">dscores</span> /= num_examples

  <span style="font-weight: bold; font-style: italic;"># </span><span style="font-weight: bold; font-style: italic;">backpropate the gradient to the parameters</span>
  <span style="font-weight: bold; font-style: italic;"># </span><span style="font-weight: bold; font-style: italic;">first backprop into parameters W2 and b2</span>
  <span style="font-weight: bold; font-style: italic;">dW2</span> = np.dot(hidden_layer.T, dscores)
  <span style="font-weight: bold; font-style: italic;">db2</span> = np.<span style="font-weight: bold;">sum</span>(dscores, axis=0, keepdims=<span style="font-weight: bold; text-decoration: underline;">True</span>)
  <span style="font-weight: bold; font-style: italic;"># </span><span style="font-weight: bold; font-style: italic;">next backprop into hidden layer</span>
  <span style="font-weight: bold; font-style: italic;">dhidden</span> = np.dot(dscores, W2.T)
  <span style="font-weight: bold; font-style: italic;"># </span><span style="font-weight: bold; font-style: italic;">backprop the ReLU non-linearity</span>
  <span style="font-weight: bold; font-style: italic;">dhidden</span>[hidden_layer &lt;= 0] = 0
  <span style="font-weight: bold; font-style: italic;"># </span><span style="font-weight: bold; font-style: italic;">finally into W,b</span>
  <span style="font-weight: bold; font-style: italic;">dW</span> = np.dot(X.T, dhidden)
  <span style="font-weight: bold; font-style: italic;">db</span> = np.<span style="font-weight: bold;">sum</span>(dhidden, axis=0, keepdims=<span style="font-weight: bold; text-decoration: underline;">True</span>)

  <span style="font-weight: bold; font-style: italic;"># </span><span style="font-weight: bold; font-style: italic;">add regularization gradient contribution</span>
  <span style="font-weight: bold; font-style: italic;">dW2</span> += reg * W2
  <span style="font-weight: bold; font-style: italic;">dW</span> += reg * W

  <span style="font-weight: bold; font-style: italic;"># </span><span style="font-weight: bold; font-style: italic;">perform a parameter update</span>
  <span style="font-weight: bold; font-style: italic;">W</span> += -step_size * dW
  <span style="font-weight: bold; font-style: italic;">b</span> += -step_size * db
  <span style="font-weight: bold; font-style: italic;">W2</span> += -step_size * dW2
  <span style="font-weight: bold; font-style: italic;">b2</span> += -step_size * db2
</pre>
</div>

<p>
训练集上的准确性：
</p>
<div class="org-src-container">

<pre class="src src-python"><span style="font-weight: bold; font-style: italic;"># </span><span style="font-weight: bold; font-style: italic;">evaluate training set accuracy</span>
<span style="font-weight: bold; font-style: italic;">hidden_layer</span> = np.maximum(0, np.dot(X, W) + b)
<span style="font-weight: bold; font-style: italic;">scores</span> = np.dot(hidden_layer, W2) + b2
<span style="font-weight: bold; font-style: italic;">predicted_class</span> = np.argmax(scores, axis=1)
<span style="color: #00bfff; font-weight: bold;">print</span> <span style="color: #daa520; font-style: italic;">'training accuracy: %.2f'</span> % (np.mean(predicted_class == y))
</pre>
</div>

<p>
可以达到 98% 的准确性，可以将整个分类平面打印出来：
</p>
<hr  />
<div class="center">

<div class="figure">
<p><img src="assets/eg/spiral_net.png" alt="spiral_net.png" />
</p>
<p><span class="figure-number">&#22270;3&nbsp;</span> 神经网络的分类平面</p>
</div>
</div>
<hr  />
</div>
</div>
</div>
</body>
</html>
