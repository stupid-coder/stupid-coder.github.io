#+TITLE: Introduction to TensorFlow's Low-Level-APIs
#+AUTHOR: stupid-coder
#+EMAIL: stupid_coder@163.com
#+STARTUP: indent
#+OPTIONS: num:nil H:2 ^:nil

原文: https://www.tensorflow.org/guide/low_level_intro

本文用来说明如何利用 TensorFlow 核心接口开发模型:
+ 需要自己接管整个模型的开发和运行,使用 *tf.Graph* 来管理计算图, *tf.Session* 来
  运行计算图.
+ 在核心接口中使用高阶组件(*datasets,layers 和 feature_columns*)
+ 不依赖 Estimator,采用核心接口构建训练流程.


推荐采用高级别接口来构建模型,但是了解核心接口如何构建模型的话,可以从多个方面更好
的理解 TensorFlow:
+ 在核心接口上进行实验和调试更为直观和灵活
+ 可以很好的理解高级别接口是如何工作的

* Tensor Values
TensorFlow 中核心数据类型为张量(/tensor/).张量就是多维矩阵, *rank* 为矩阵的维
度数量,形状标示了多维矩阵在维度上的大小:
#+BEGIN_SRC python
  3. # a rank 0 tensor; a scalar with shape [],
  [1., 2., 3.] # a rank 1 tensor; a vector with shape [3]
  [[1., 2., 3.], [4., 5., 6.]] # a rank 2 tensor; a matrix with shape [2, 3]
  [[[1., 2., 3.]], [[7., 8., 9.]]] # a rank 3 tensor with shape [2, 1, 3]
#+END_SRC

TensorFlow 采用 NumPy 数组来表示内部张量的实际值.

* TensorFlow Core Walkthrough
可以认为 TensorFlow 核心接口编程由两部分组成:
+ 构建计算图(*tf.Graph*)
+ 运行计算图(*tf.Session*)

** Graph
计算图(/computational graph/)是由多个 TensorFlow 操作符排列构成的,主要为两种
类型的对象组成:
+ =tf.Operation= :: 计算图中的节点,Operation 描述了输入 Tensors 和产出 Tensors 的
                    计算过程.
+ =tf.Tensor= :: 图中的边.表示了在图中流动的数据对象.绝大多数的 TensorFlow 方法
                 返回 *tf.Tensors*


#+BEGIN_QUOTE
需要记住的是 tf.Tensors 并不是真正的包含数据,而是代表了计算图中的数据对象.
#+END_QUOTE

构建一个简单的计算图.最基础的的操作符为常量.通过调用重载的加法函数接受两个
tensor,输入一个 tensor:
#+BEGIN_SRC python
  a = tf.constant(3.0, dtype=tf.float32)
  b = tf.constant(4.0) # also tf.float32 implicitly
  total = a + b
  print(a)
  print(b)
  print(total)
#+END_SRC

上述代码输出:
#+BEGIN_SRC python
  Tensor("Const:0", shape=(), dtype=float32)
  Tensor("Const_1:0", shape=(), dtype=float32)
  Tensor("add:0", shape=(), dtype=float32)
#+END_SRC

上述打印张量,并没有输出希望的值 3.0, 4.0 和 7.0.上述的代码只是构建了计算图,这些
*tf.Tensor* 对象只是代表了计算图中的操作符执行的结果.

每一个计算图中的操作符都具有一个唯一的名字.这个名字和 python 中的变量名无关.张
量采用操作符和输出的索引来命名, (add:0) 如上.

** TensorBoard
TensorFlow 提供了工具 *TensorBoard*,该工具主要的一个功能是可视化计算图.

首先需要调用如下命令来保存计算图到 *TensorBoard* 文件中:
#+BEGIN_SRC python
  writer = tf.summary.FileWriter('.')
  writer.add_graph(tf.get_default_graph())
#+END_SRC

如上代码会在当前目录下生成一个 *event* 文件,命名规则如下:
#+BEGIN_SRC sh
  instance_events.out.tfevents.{timestamp}.{hostname}
#+END_SRC

然后在终端中调用 TensorBoard:
#+BEGIN_SRC sh
  tensorboard --logdir .
#+END_SRC

即可查看对应的可视化结果.

** Session
通过实例化 *tf.Session* 对象,该对象主要功能可以存储 TensorFlow 运行过程中的状态,并
且用来执行计算图. *tf.Graph* 类似 .py 文件, *tf.Session* 类似 python 解释器.

如下代码创建 *tf.Session* 对象,然后调用对应的 *run* 方法执行 total 张量,从而执
行实际的计算:
#+BEGIN_SRC python
     sess = tf.Session()
     print(sess.run(total))
#+END_SRC

*Session.run* 会根据需要计算的节点,从图上回溯需要计算的所有节点,然后送入对应
的参数,执行需要的计算,最终返回计算结果.所以上述代码会返回对应的希望结果 7.0:
#+BEGIN_SRC javascript
      7.0
#+END_SRC

*tf.Session.run* 可以接受多个张量作为参数.并且支持元素和字典:
#+BEGIN_SRC python
      print(sess.run({'ab':(a, b), 'total':total}))
#+END_SRC

上述代码输出和输入结构一样:
#+BEGIN_SRC javascript
      {'total': 7.0, 'ab': (3.0, 4.0)}
#+END_SRC

在每次 *tf.Session.run* 调用过程中,计算图中的任意 *tf.Tensor* 都能只有一个唯
一的值.例如:如下代码调用 *tf.random_uniform* 返回随机的 *tf.Tensor* :
#+BEGIN_SRC python
      vec = tf.random_uniform(shape=(3,))
      out1 = vec + 1
      out2 = vec + 2
      print(sess.run(vec))
      print(sess.run(vec))
      print(sess.run((out1, out2)))
#+END_SRC

结果显示每次调用 run 方法,结果都是不同的,但是在同一次 run 方法调用中随机值是固
定的(*out1* 和 *out2* 接受同一个输入):
#+BEGIN_SRC javascript
      [ 0.52917576  0.64076328  0.68353939]
      [ 0.66192627  0.89126778  0.06254101]
      (
          array([ 1.88408756,  1.87149239,  1.84057522], dtype=float32),
          array([ 2.88408756,  2.87149239,  2.84057522], dtype=float32)
      )
#+END_SRC

有一些 TensorFlow 函数返回的是 *tf.Operations*.run 方法以 operation 作为参数调用
的时候,返回结果为 None,但是会执行相关的副作用.例如: [[*Initializing Layers][Initializing Layers]]和
[[*Training][Training]]相关操作符.

** Feeding
计算图也可以接受参数作为输入,从而实现计算图的参数特化,该参数叫做占位符
(/placeholders/).占位符只是在计算图中占领一个节点,并且保证在后续实际计算图运
算时提供一个值作为输入.
#+BEGIN_SRC python
     x = tf.placeholder(tf.float32)
     y = tf.placeholder(tf.float32)
     z = x + y
#+END_SRC

上述三行代码类似一个函数,定义了两个输入参数(/x,y/),然后执行一个操作.可以通过
在调用 *tf.Session.run* 方法时,将实际的值喂给对应的 *feed_dict* 参数,实现将值
喂给计算图中进行实际计算:
#+BEGIN_SRC python
     print(sess.run(z, feed_dict={x: 3, y: 4.5}))
     print(sess.run(z, feed_dict={x: [1, 3], y: [2, 4]}))
#+END_SRC

结果如下:
#+BEGIN_SRC javascript
     7.5
     [ 3.  7.]
#+END_SRC

feed_dict 参数可以用来覆盖计算图中任意的 tensor 值.placehodler 和 tensor 不同的地方
在于,在执行计算图的时候,placeholder 对象必须赋予实际的值,非则会抛出对应的异常
错误.
   
* Datasets
占位符只适合执行小规模实验, *tf.data* 可以支持流式数据输入.

为了从 Dataset 中获取可以运行的 *tf.Tensor*,首先需要从 Dataset 中获取对应的迭代器
*tf.data.Iterator*,然后调用迭代器的 *tf.data.Iterator.get_next* 方法来获得对应
的数据.

最简单创建迭代器的方法是调用 *tf.data.Dataset.make_one_shot_iterator* 方法.如
下代码中, *next_item* 张量每次运行会从 my_data 数组中返回一行数据:
#+BEGIN_SRC python
    my_data = [
        [0, 1,],
        [2, 3,],
        [4, 5,],
        [6, 7,],
    ]
    slices = tf.data.Dataset.from_tensor_slices(my_data)
    next_item = slices.make_one_shot_iterator().get_next()
#+END_SRC

Dataset 会在数据读取完成后,抛出 *tf.errors.OutOfRangeError* 错误.例如:如下代码
会一直读取 next_item,直到数据集没有数据:
#+BEGIN_SRC python
    while True:
      try:
        print(sess.run(next_item))
      except tf.errors.OutOfRangeError:
        break
#+END_SRC

更为详细的数据集细节可以参考[[https://www.tensorflow.org/guide/datasets][importing data]].

* Layers
可训练的模型主要是通过微调计算图中的某些权值变量,从而实现相同的输入,执行计算图
中计算后能够获得不同的输出. *tf.layers* 是优先采用的方法,将可训练参数加到计算
图中.

网络层将可训练变量和对应执行的计算封装在一起.例如: *densely-connected layer*
用来执行输入值和权值点乘,然后执行激活函数.可训练的权值和偏置由网络层负责添加到
计算图中.

** Creating Layers
如下代码用来创建 *tf.layers.Dense* 网络层,接受 3 维的向量,然后生成一个单一节点:
#+BEGIN_SRC python
     x = tf.placeholder(tf.float32, shape=[None, 3])
     linear_model = tf.layers.Dense(units=1)
     y = linear_model(x)
#+END_SRC

网络层通过输入来决定内部的权重值维度.所以这里需要设置输入的占位符 x 的形状,从而
使得网络层可以构建内部权值矩阵.

上述定义了如何计算输入张量 *y* ,在执行计算前,还需要初始化网络层.

** Initializing Layers
网络层中包含一些变量,需要初始化后才能使用.TensorFlow 可以单个变量初始化,也可以
直接初始化整个计算图:
#+BEGIN_SRC python
     init = tf.global_variables_initializer()
     sess.run(init)
#+END_SRC

由于 *global_variables_initializer* 返回初始化操作符,该操作符只包含了调用时计
算图中需要初始化的变量.所以初始化操作应该是最后添加到计算图中的操作符.

** Executing Layers
网络层初始化后,就可以运行 *linear_model* 输出的张量:
#+BEGIN_SRC python
     print(sess.run(y, {x: [[1, 2, 3],[4, 5, 6]]}))
#+END_SRC

** Layer Function shortcuts
对于任意的网络层类(例如: tf.layers.Dense) TensorFlow 同时提供了一个快捷函数用来
快速构建对应的网络图(*tf.layers.dense*).快捷函数将网络层的创建和添加到计算图中集
中在一行代码中.例如:
#+BEGIN_SRC python
  x = tf.placeholder(tf.float32, shape=[None, 3])
  y = tf.layers.dense(x, units=1)  # 网络层创建和添加到计算图中

  init = tf.global_variables_initializer()
  sess.run(init)

  print(sess.run(y, {x: [[1, 2, 3], [4, 5, 6]]}))
#+END_SRC

快捷函数虽然可以将两部分操作合并在一个函数中.但是无法使得网络层重复使用,且不利于
调试.

* Feature columns
*feature columns* 为 TensorFlow 用来支持特征解析和变换的.最简单的使用就是通过定
义对应的 *feature columns* 解析对象后,通过调用 *tf.feature_column.input_layer*
方法来执行对应的特征解析和变换,该方法只接受 *dense columns* 作为输入,所以
*categorical column* 必须通过 *tf.feature_column.indicator_column* 转换成 *dense
column* 才能使用:
#+BEGIN_SRC python
  features = {
      'sales' : [[5], [10], [8], [9]],
      'department': ['sports', 'sports', 'gardening', 'gardening']}

  department_column = tf.feature_column.categorical_column_with_vocabulary_list(
          'department', ['sports', 'gardening'])
  department_column = tf.feature_column.indicator_column(department_column)

  columns = [
      tf.feature_column.numeric_column('sales'),
      department_column
  ]

  inputs = tf.feature_column.input_layer(features, columns)
#+END_SRC

执行 inputs 张量,会解析 features 存储的批次特征值.

*feature columns* 内部包含状态,类似网络层,所以需要初始化后才能使用.categorical
columns 内部调用 *tf.contrib.lookup* 来查看对应的种类信息,所以需要额外的初始化操
作 *tf.tables_initializer*.
#+BEGIN_SRC python
  var_init = tf.global_variables_initializer()
  table_init = tf.tables_initializer()
  sess = tf.Session()
  sess.run((var_init, table_init))
#+END_SRC

初始化完成之后,就可以运行 inputs 张量来获取对应的结果值:
#+BEGIN_SRC python
  print(sess.run(inputs))
#+END_SRC

* Training
上述基本已经熟悉了 TensorFlow 核心接口基础知识,下面来手动训练一个简单的回归模型.

** Define the data
首先定义输入 x,和希望输出 y_true:
#+BEGIN_SRC python
  x = tf.constant([[1], [2], [3], [4]], dtype=tf.float32)
  y_true = tf.constant([[0], [-1], [-2], [-3]], dtype=tf.float32)
#+END_SRC

** Define the model
接着构建一个简单的线性模型:
#+BEGIN_SRC python
  linear_model = tf.layers.Dense(units=1)

  y_pred = linear_model(x)
#+END_SRC

然后就可以直接进行评估预测结果:
#+BEGIN_SRC python
  sess = tf.Session()
  init = tf.global_variables_initializer()
  sess.run(init)

  print(sess.run(y_pred))
#+END_SRC

由于模型没有经过训练所以结预测的结果并不会太好.

** Loss
为了优化模型,首先需要定义损失函数.这里采用标准的回归模型采用的损失函数: *均值平方差*.

*tf.losses* 模块中提供了很多常用的损失函数,如下为对应的均值平方差错误:
#+BEGIN_SRC python
  loss = tf.losses.mean_squared_error(labels=y_true, predictions=y_pred)

  print(sess.run(loss))
#+END_SRC

** Training
TensorFlow 提供了很多优化算法,所有的优化算法都是 *tf.train.Optimizer* 的子类实现.通
过调整计算图中的可训练的权值来最小化损失函数.最简单的优化算法是梯度下降,具体实现
为 *tf.train.GradientDescentOptimizer*,通过求解参数的梯度值,然后朝反方向进行调整:
#+BEGIN_SRC python
  optimizer = tf.train.GradientDescentOptimizer(0.01)
  train = optimizer.minimize(loss)
#+END_SRC

上述代码将优化算法需要执行的所有计算操作符都添加到计算图中,并返回对应的训练操作
符.当执行该操作符的时候,计算会根据优化算法更新计算图中的可训练变量.
#+BEGIN_SRC python
  for i in range(100):
      _, loss_value = sess.run((train, loss))
      print(loss_value)
#+END_SRC

由于 *train* 是一个操作符,而非 tensor,所以返回结果为 None.
